# Introducci√≥n

## ¬øQu√© son los Modelos gr√°ficos probabil√≠sticos?

No vamos a comenzar con una definici√≥n formal, sino que vamos a descomponer _<<Modelos gr√°ficos probabil√≠sticos>>_ en sus partes constitutivas.

Para comenzar, la **Figura 1** presenta de forma gr√°fica una s√≠ntesis de los conceptos fundamentales que subyacen al enfoque de los Modelos gr√°ficos probabil√≠sticos (MGP).

![Conceptos fundamentales de los MGP](../source/images/sesion1-conceptosclave.png)

**Figura 1.** Componentes clave de los MGP. Este enfoque combina una representaci√≥n declarativa (modelo), una estructura basada en grafos para modelar sistemas complejos (gr√°fico), y un marco formal para razonar e inferir bajo incertidumbre (probabil√≠stico).

```{admonition} ¬øA qu√© nos referimos con "Modelos"
:class: note

Un "modelo" es una _representaci√≥n declarativa_ de nuestro entendimiento del mundo.

* Proponemos un modelo que captura _c√≥mo entendemos las variables_ que describen un fen√≥meno y c√≥mo interact√∫an entre s√≠.

* Que sea _declarativo_ significa que la representaci√≥n se sostiene por s√≠ misma. Esto quiere decir que podemos dar sentido al modelo _independientemente_ del _algoritmo_ que elijamos aplicar sobre √©l.
```

```{admonition} ¬øA qu√© nos referimos con "Probabil√≠sticos"?
:class: note

La palabra "probabil√≠sticos" est√° presente porque nos ayuda a manejar la _incertidumbre_.

* La _teor√≠a de la probabilidad_ es un marco formal que nos permite representar la _incertidumbre_ de forma coherente y cuantitativa.

* Podemos:
    - Razonar con nueva informaci√≥n --> _qu√© pasa si observamos algo nuevo?_

    - Tomar decisiones bajo incertidumbre --> _qu√© es lo mejor que podemos hacer?_

    - Aprender modelos desde los datos --> gracias a la conexi√≥n con la estad√≠stica, la probabilidad nos permite aplicar t√©cnicas de aprendizaje autom√°tico estad√≠stico para aprender los par√°metros o la estructura de un modelo.
```

```{admonition} ¬øA qu√© nos referimos con "Gr√°ficos"?
:class: note

La palabra "gr√°ficos" est√° presente porque nos ayuda a manejar la _complejidad_.

* Un grafo nos ayudar√° a representar sistemas complejos de forma modular y visual.

* Un grafo es una estructura matem√°tica compuesta por nodos (o v√©rtices) y aristas (o enlaces) que conectan pares de nodos. En el contexto de los MGP, los nodos representan variables aleatorias y las aristas representan dependencias probabil√≠sticas entre estas variables.
```

Citando a Michael I. Jordan (1998):

> _Graphical models are a marriage between probability theory and graph theory. They provide a natural tool for dealing with two problems that occur throughout applied mathematics and engineering -- **uncertainty and complexity** -- and in particular they are playing an increasingly important role in the design and analysis of machine learning algorithms. Fundamental to the idea of a graphical model is the notion of **modularity** -- a complex system is built by combining simpler parts. Probability theory provides the glue whereby the parts are combined, ensuring that the system as a whole is consistent, and providing ways to interface models to data. The graph theoretic side of graphical models provides both an intuitively appealing interface by which humans can model highly-interacting sets of variables as well as a data structure that lends itself naturally to the design of efficient general-purpose algorithms._

> _Many of the classical multivariate probabalistic systems studied in fields such as statistics, systems engineering, information theory, pattern recognition and statistical mechanics are special cases of the general graphical model formalism -- examples include mixture models, factor analysis, hidden Markov models, Kalman filters and Ising models. The graphical model framework provides a way to view all of these systems as instances of a common underlying formalism. This view has many advantages -- in particular, specialized techniques that have been developed in one field can be transferred between research communities and exploited more widely. Moreover, the graphical model formalism provides a natural framework for the design of new systems."_

[Citado por Kevin Murphy, 1998.](https://www.cs.ubc.ca/%7Emurphyk/Bayes/bnintro.html) y cita original de Michael I. Jordan, 1998 en _Learning in Graphical Models_.

---

## Entonces...

**¬øPor qu√© aprender un nuevo marco conceptual ‚Äîy no solo otro modelo m√°s‚Äî para el an√°lisis en Machine Learning?**

Es el momento de conectar con el curso üíô y descubrir el valor profundo de dominar este marco conceptual.

En ciencia de datos usamos todo tipo de modelos: redes neuronales, √°rboles de decisi√≥n, regresiones, random forests, etc. Entonces, **¬øpor qu√© dedicar un curso completo a los modelos gr√°ficos probabil√≠sticos?**

> Porque abordan _otro tipo de preguntas_ y _otro tipo de problemas_ que no son tan f√°ciles de resolver con los modelos tradicionales.

```{admonition} ¬øqu√© hace √∫nicos a los MGP?
:class: important

Los MGP **separan claramente el modelo del algoritmo**.

- **Formular distintas preguntas sobre el mismo modelo**:

  - Diagn√≥stico: $P(\text{Enfermedad} \mid \text{S√≠ntomas})$
  - Predicci√≥n: $P(\text{S√≠ntomas} \mid \text{Enfermedad})$
  - Completado de datos faltantes: $P(X_i \mid X_{-i})$
  - Detecci√≥n de anomal√≠as: ¬øcu√°nto se desv√≠a una observaci√≥n del comportamiento esperado?

- **Elegir distintos algoritmos seg√∫n necesidad**:

  - Inferencia exacta (si el modelo es peque√±o)
  - Inferencia aproximada (si es grande o intractable)
  - Algoritmos m√°s r√°pidos pero menos precisos, o m√°s costosos pero m√°s exactos.

- **Dise√±ar modelos m√°s interpretables y modulares**.
```

En la mayor√≠a de los modelos de machine learning, el **modelo** y el **algoritmo** vienen **"pegados"**: solo sirven para una tarea espec√≠fica.

Pero en MGP, puedes _mantener el modelo fijo_ y _cambiar el algoritmo_ seg√∫n la pregunta que quieras hacer.

> Eso es lo que los convierte no solo en una herramienta, sino en un **marco de razonamiento general bajo incertidumbre**.

![modelo representacion](../source/images/sesion1-integracion.png)

**Figura 2.** El modelo como representaci√≥n declarativa. Un modelo gr√°fico probabil√≠stico puede construirse a partir de conocimiento experto (elicitaci√≥n) o aprendido a partir de datos, y luego ser manipulado mediante distintos algoritmos seg√∫n el tipo de pregunta o tarea. Esta separaci√≥n entre modelo y algoritmo es una de las ventajas clave del enfoque. Inspirado en material del curso _Probabilistic Graphical Models 1: Representation_, impartido por Daphne Koller (Stanford University).

---

## Tres pilares de los MGP:

Seg√∫n _Daphne Koller_ y _Nir Friedman_, los MGP se basan en tres pilares fundamentales:

> _‚ÄúThese three components ‚Äî **representation, inference, and learning** ‚Äî are critical components in constructing an intelligent system. We need a **declarative representation** that is a reasonable encoding of our world **model**. We need to be able to use this representation effectively to answer a broad range of **questions** that are of interest. And we need to be able to acquire this distribution, combining **expert knowledge** and **accumulated data**. Probabilistic graphical models are one of a small handful of frameworks that support all three capabilities for a broad range of problems.‚Äù_

### I. Representaci√≥n

```{figure} ../source/images/nodo.png
:alt: representacion
:fig-align: center
:width: 200px

```

> Usar una estructura gr√°fica para capturar dependencias entre variables aleatorias.

- El _grafo_ hace expl√≠cita la estructura causal o condicional entre variables.

- Esta representaci√≥n es declarativa y transparente, es decir:

  - Se puede inspeccionar.

  - Un humano puede interpretarla.

  - Se separa del algoritmo: podemos usar distintos algoritmos sobre el mismo modelo.

---

### II. Inferencia

```{figure} ../source/images/lupa.png
:alt: representacion
:fig-align: center
:width: 200px

```

> Aplicar _algoritmos de inferencia_ sobre el modelo para responder _preguntas_.

- Por ejemplo:

  - ¬øCu√°l es la probabilidad de que un paciente tenga gripe dado que tiene fiebre?

  - ¬øQu√© s√≠ntomas debo observar para reducir mi incertidumbre?

- Los algoritmos de inferencia usan la estructura gr√°fica para responder estas consultas eficientemente, sin necesidad de reconstruir toda la distribuci√≥n conjunta.

- Pueden ser exactos _(variable elimination, belief propagation)_ o aproximados _(sampling, variational)_.

---

### III. Aprendizaje

```{figure} ../source/images/cerebro.png
:alt: representacion
:fig-align: center
:width: 200px

```

> Construir el modelo autom√°ticamente a partir de datos hist√≥ricos.

- Esto incluye:

  - Estimar par√°metros (probabilidades condicionales).

  - Aprender la estructura del grafo.

- Se apoya en enfoques impulsados por datos (data-driven).

- Resultado: modelos que reflejan mejor el dominio real que los modelos construidos puramente a mano.

- Combinan conocimiento experto con informaci√≥n emp√≠rica.

---

![](../source/images/sesion1-graphmodel.png)

_Figura 3._ Ejemplo de un modelo gr√°fico probabil√≠stico.

Con lo visto hasta ahora, vamos a definir los objetivos del curso.

## Objetivo general

- Comprender los modelos gr√°ficos probabil√≠sticos y sus aplicaciones en el an√°lisis de datos, la inferencia estad√≠stica y la toma de decisiones en entornos de incertidumbre mediante la construcci√≥n, an√°lisis y aplicaci√≥n de modelos como redes bayesianas y redes de Markov para modelar sistemas complejos y resolver problemas pr√°cticos en diversas √°reas.

### Objetivos particulares

- Modelar problem√°ticas/situaciones reales mediante grafos probabil√≠sticos.

- Responder preguntas de inter√©s pr√°ctico acerca de las problem√°ticas/situaciones modeladas usando algoritmos de inferencia.

- Estimar los par√°metros y/o estructura de los modelos usando t√©cnicas de optimizaci√≥n.

### Prerrequisitos

- Probabilidad y Estad√≠stica ‚Äì üé≤
- Programaci√≥n en Python ‚Äì üêç
- Optimizaci√≥n Convexa ‚Äì üìà
- An√°lisis Estad√≠stico Multivariado üìä

## Referencias

- Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. MIT Press.

- Jordan, M. I. (1998). Learning in Graphical Models. _NATO Science Series D: Behavioural and Social Sciences_ vol. 89. Springer.
