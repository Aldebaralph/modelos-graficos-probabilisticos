# Sesi√≥n 2

## 1. Repaso de probabilidad

### 1.1. Introducci√≥n

Antes de sumergirnos en el mundo de los _modelos gr√°ficos probabil√≠sticos_, comprendamos primero algunos de los fundamentos de la teor√≠a de probabilidad, ya que constituye la base formal para este enfoque.

Una forma de visualizar su importancia es observando c√≥mo diferentes perspectivas epistemol√≥gicas y metodol√≥gicas dan lugar a distintos tipos de modelado. En la **Figura 1**, se muestra una distinci√≥n entre dos enfoques principales:

- El **enfoque determinista**, que se basa en el modelado no probabil√≠sta y utiliza m√©todos donde no interviene el azar.

- El **enfoque estoc√°stico**, que recurre al modelado probabil√≠sta y se sustenta formalmente en la teor√≠a de probabilidad.

Ambos caminos convergen en aplicaciones pr√°cticas como los **modelos de aprendizaje autom√°tico (ML) y aprendizaje profundo (DL)**. Estos modelos pueden surgir tanto desde una perspectiva determinista como estoc√°stica, aunque es en el enfoque probabil√≠stico donde encontramos herramientas m√°s directas para representar la _incertidumbre_ y tomas decisiones informadas.

![](../images/sesion2-esquema.png)

**Figura 1.** Elaboraci√≥n propia.

### 1.2. Fundamentos de la teor√≠a de probabilidad

La teor√≠a de probabilidad nos permite modelar incertidumbre mediante una estructura matem√°tica compuesta por tres elementos: _un espacio muestral $\Omega$, un sistema de eventos medibles $S$ y una funci√≥n de probabilidad $\mathbb{P}$_. Formalmente, esta estructura se expresa como la **triple**:

$$(\Omega, S, \mathbb{P})$$

---

A continuaci√≥n, desglosamos cada uno de estos componentes:

#### 1.2.1. Espacio muestral $(\Omega)$

El **espacio muestral** es el conjunto que contiene **todos los posibles resultados** de un experimento aleatorio. Se denota por:

$$
\Omega
$$

- Representa **todo lo que puede pasar**.

- Es el punto de partida para definir eventos.

```{admonition} Ejemplo
:class: tip

Si lanzamos un dado de 6 caras, el espacio muestral es:

$\Omega = \{1, 2, 3, 4, 5, 6\}$
```

Puedes consultar m√°s [aqu√≠](https://en.wikipedia.org/wiki/Probability_space)

#### 1.2.2. Conjunto potencia $\mathcal{P}(\Omega)$

Es el conjunto de **todos los subconjuntos _posibles_** de $\Omega$.

- Incluye desde el conjunto vac√≠o $\varnothing$ hasta el conjunto total $\Omega$.

- Representa **todas las _combinaciones_ posibles de eventos.**

```{admonition} Ejemplo
:class: tip

Si $\Omega={1,2,3}$, entonces:

$P(\Omega) = \{\varnothing, \{1\}, \{2\}, \{3\}, \{1,2\}, \{1,3\}, \{2,3\}, \{1,2,3\}\}$
```

A continuaci√≥n, podemos ver el **conjunto potencia** en la **Figura 2**, representado mediante un diagrama de Hasse.

![](../images/hasse.png)

**Figura 2.** Diagrama de Hasse del conjunto potencia de ${1,2,3}$. Elaboraci√≥n propia con base en: [_Power set_](https://en.wikipedia.org/wiki/Power_set).

Puedes consultar m√°s [aqu√≠](https://en.wikipedia.org/wiki/Power_set).

#### 1.2.3. œÉ-√°lgebra $(S)$ o espacio de eventos

Es una colecci√≥n especial de subconjuntos de $\Omega$ (es decir, una subcolecci√≥n de $\mathcal{P}(\Omega)$) que cumple con tres propiedades clave:

```{admonition} Propiedades del sistema de eventos $S$
:class: tip

1. **Contiene el evento vac√≠o y el evento total**
   - $\varnothing \in S$: representa el evento que nunca ocurre.
   - $\Omega \in S$: representa el evento que siempre ocurre.

2. **Cerrado bajo uni√≥n**
   Si $\alpha, \beta \in S$, entonces $\alpha \cup \beta \in S$.
   Esto permite formar eventos como ‚Äúocurre $\alpha$ o $\beta$‚Äù.

3. **Cerrado bajo complemento**
   Si $\alpha \in S$, entonces $\Omega - \alpha \in S$ (tambi√©n denotado $\alpha^c$).
   Esto garantiza que tambi√©n podamos trabajar con el evento ‚Äúno ocurre $\alpha$‚Äù.
```

```{admonition} Ejemplo
Si $\Omega={1,2,3}$, una posible œÉ-√°lgebra es:

$$S = \{\varnothing, \{1,2,3\}, \{1\}, \{2,3\}\}$$

Aqu√≠, $\{2,3\}$ es el complemento de $\{1\}$, y viceversa.
```

Puedes consultar m√°s [aqu√≠](https://en.wikipedia.org/wiki/%CE%A3-algebra#Definition_and_properties).

#### 1.2.4. Funci√≥n de probabilidad

Hasta ahora hemos definido el espacio muestral $(\Omega)$, que contiene todos los posibles resultados de un experimento, y una œÉ-√°lgebra $(S)$, que representa los subconjuntos medibles de $(\Omega)$, es decir, los **eventos** a los que podemos asignar una probabilidad coherente.

Una **funci√≥n de probabilidad** es una regla matem√°tica que asigna a cada evento medible un n√∫mero entre $0$ y $1$, representando **cu√°n probable** es que ese evento ocurra. Formalmente:

$$\mathbb{P} : S \rightarrow [0, 1]$$

o

$$0 \leq \mathbb{P}(\text{evento}) \leq 1$$

Para que esta asignaci√≥n tenga sentido y sea consistente con la intuici√≥n, la funci√≥n $\mathbb{P}$ debe cumplir tres condiciones fundamentales, conocidas como los **axiomas de Kolmog√≥rov**:

```{admonition} Definici√≥n formal
:class: tip

Una funci√≥n $\mathbb{P}$ es una **probabilidad** sobre el espacio $(\Omega, S)$ si cumple:

1. **No negatividad:** $\mathbb{P}(A) \geq 0$ para todo $A \in S$

2. **Normalizaci√≥n:** $\mathbb{P}(\Omega) = 1$

3. **Aditividad numerable:** si tienes una **colecci√≥n infinita de eventos** $(A_1, A_2, A_3, \ldots)$ que son **mutuamente excluyentes** (es decir, no se superponen, o sea, $(A_i \cap A_j = \emptyset)$ si $( i \ne j )$), entonces la probabilidad de que ocurra alguno de esos eventos (la uni√≥n de todos ellos) es igual a la **suma de las probabilidades individuales**.

$$
\mathbb{P}\left( \bigcup_{i=1}^{\infty} A_i \right) = \sum_{i=1}^{\infty} \mathbb{P}(A_i)
$$

Esto garantiza que la probabilidad se comporta de manera coherente incluso cuando se trata de **infinitas situaciones posibles**, no solo finitas. Es una caracter√≠stica esencial para que una funci√≥n se considere una **medida de probabilidad** en _teor√≠a de la medida_.
```

> üìå _Nota:_ Estos tres principios consolidan la probabilidad como una rama formal de la matem√°tica, basada en la _teor√≠a de conjuntos_ y la _teor√≠a de la medida_.

```{admonition} Evento en teor√≠a de probabilidad
:class: tip

En teor√≠a de probabilidad, un **evento** es cualquier subconjunto del espacio muestral $\Omega$ que **pertenece a la œÉ-√°lgebra** $(S)$.
Solo a estos eventos se les puede asignar una probabilidad formalmente v√°lida.
Por eso tambi√©n se les llama **eventos medibles**.
```

##### ¬øC√≥mo se relacionan los conceptos que hemos visto?

Para construir un modelo probabil√≠stico s√≥lido, necesitamos entender c√≥mo se relacionan tres objetos fundamentales: el **espacio muestral**, el **conjunto potencia** y la **œÉ-√°lgebra**.

- El **espacio muestral** $\Omega$ es el punto de partida: contiene todos los posibles resultados de un experimento.

- A partir de √©l, podemos formar el **conjunto potencia** $\mathcal{P}(\Omega)$, que incluye **todos los subconjuntos posibles** de $\Omega$. En principio, cada uno de estos subconjuntos podr√≠a considerarse un evento.

- Sin embargo, no todos los subconjuntos de $\Omega$ pueden ser tratados como **eventos v√°lidos** desde el punto de vista de la probabilidad. Para que un subconjunto sea un **evento medible**, debe pertenecer a una **œÉ-√°lgebra** $S \subseteq \mathcal{P}(\Omega)$, la cual cumple ciertas propiedades de consistencia l√≥gica (como estar cerrada bajo uni√≥n, complemento, etc.).

Podemos resumir estas diferencias clave en la siguiente tabla:

| Concepto                                | Qu√© representa                                | Contenido                                    |
| --------------------------------------- | --------------------------------------------- | -------------------------------------------- |
| Espacio muestral $(\Omega)$             | Resultados posibles de un experimento         | Un conjunto base (como $\{1,2,3\})$          |
| Conjunto potencia $\mathcal{P}(\Omega)$ | Todos los subconjuntos posibles de $(\Omega)$ | Todos los eventos posibles, medibles o no    |
| $\sigma$-√°lgebra $(S)$                  | Subconjuntos **medibles** de $(\Omega)$       | Subconjuntos que cumplen ciertas propiedades |

---

La siguiente **Figura 2** ilustra visualmente c√≥mo se relacionan estos tres niveles de generalidad:

![](../images/all.jpg)

**Figura 2.** Relaci√≥n entre espacio muestral, conjunto potencia y $\sigma$-√°lgebra. Elaboraci√≥n propia.

#### 1.2.5. Distribuci√≥n de probabilidad

Una vez definida la funci√≥n de probabilidad $\mathbb{P}$, podemos describir **c√≥mo se reparte esa probabilidad** entre los posibles resultados de un experimento. A esto lo llamamos **distribuci√≥n de probabilidad**.

Dependiendo del tipo de espacio muestral, la distribuci√≥n puede tomar dos formas principales:

---

##### 1.2.5.1 Distribuciones discretas

![](../images/discrete-distr.png)

Este tipo de distribuci√≥n se utiliza cuando el _espacio muestral_ $\Omega$ es **finito o numerable** (como lanzar un dado o una moneda).

Asignan una probabilidad expl√≠cita a cada valor individual de la variable aleatoria:

$$
\mathbb{P}(X = x_i) = p_i \quad \text{con } \sum_i p_i = 1
$$

```{admonition} Ejemplo
:class: tip

Supongamos que lanzamos un dado justo.
Entonces $\Omega = \{1, 2, 3, 4, 5, 6\}$ y usamos $\mathcal{P}(\Omega)$ como $\sigma$-√°lgebra.

Definimos la probabilidad como:

$$
\mathbb{P}(\{i\}) = \frac{1}{6} \quad \text{para } i = 1,\dots,6
$$

As√≠, por ejemplo:
- $\mathbb{P}(\{2,4,6\}) = \frac{3}{6} = 0.5$
- $\mathbb{P}(\{1\}) = \frac{1}{6}$
```

---

##### 1.2.5.1 Distribuciones continuas

![](../images/cont-distr.png)

Se definen sobre espacios infinitos (por ejemplo, $\mathbb{R}$).
No asignan probabilidad a valores individuales, sino a intervalos, mediante una funci√≥n de densidad $f(x)$:

$$
\mathbb{P}(a \leq X \leq b) = \int_a^b f(x)\, dx
$$

```{admonition} Ejemplo
:class: tip

La distribuci√≥n normal (o gaussiana) tiene densidad:

$$
f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}
$$

Y cumple:

- $f(x) \geq 0$ para todo $x$
- $\int_{-\infty}^{\infty} f(x) \, dx = 1$

üí° *Nota:* $f(x)$ no representa una probabilidad directa, sino una **densidad**:
la probabilidad de que $X$ tome un valor exacto (como $X = 0$) es **0**. Solo intervalos tienen probabilidad positiva.
```

```{admonition} Nota
:class: note

Las _distribuciones de probabilidad_ resumen toda la informaci√≥n necesaria para calcular eventos como:
$\mathbb{P}(X \in A)$, para cualquier conjunto medible $A \in S$.

- En el caso **continuo**, esto se hace mediante **integrales** sobre funciones de densidad.
- En el caso **discreto**, mediante **sumas** de probabilidades individuales.
```

> üìå _Nota:_ Puedes explorar gr√°ficamente las diferentes distribuciones en [este recurso.](https://seeing-theory.brown.edu/probability-distributions/index.html#section1)

#### 1.2.6. Interpretaciones de la probabilidad

Ya sabemos qu√© es una _funci√≥n de probabilidad_ y c√≥mo puede _distribuirse_ sobre un espacio muestral. Es importante notar que, en este punto, la _probabilidad_ es un objeto estrictamente matem√°tico, cuya definici√≥n est√° separada del proceso mediante el cual se asignan probabilidades a los eventos.

Pero surge una pregunta fundamental:

> **¬øC√≥mo se asignan o calculan esos valores de probabilidad en la pr√°ctica?**

La respuesta depende de la **interpretaci√≥n** que adoptemos sobre qu√© representa una probabilidad en un contexto real.

A continuaci√≥n, exploramos dos de las interpretaciones m√°s influyentes y ampliamente utilizadas: la **frecuentista** y la **bayesiana**.

##### Enfoque frecuentista: medir por repetici√≥n

> Si repetimos un experimento muchas veces, el cociente entre el n√∫mero de veces que ocurre un evento y el total de repeticiones se usa como una estimaci√≥n de su probabilidad. Este valor es lo que se conoce como la _frecuencia relativa_.

Este valor se llama _frecuencia relativa_ del evento:

$$\mathbb{P}(A) \approx \frac{\#A}{n}$$

A medida que el n√∫mero de experimentos crece, esta frecuencia relativa tiende (bajo ciertas condiciones) a estabilizarse en un valor fijo. Este valor es interpretado como la **probabilidad** del evento desde el punto de vista _frecuentista_.

```{admonition} Frecuencia relativa
:class: attention

La **frecuencia relativa** es el cociente entre el n√∫mero de veces que ocurre un evento y el n√∫mero total de repeticiones del experimento. En el enfoque **frecuentista**, esta proporci√≥n se interpreta como la probabilidad del evento, especialmente cuando el n√∫mero de repeticiones es grande.
```

```{admonition} Ejemplo
:class: tip

![](../images/sesion2-freq.png)

Si lanzamos una moneda 1000 veces y cae cara en 502 de ellas, entonces:

$$
\mathbb{P}(\text{cara}) \approx \frac{502}{1000} = 0.502
$$

Al aumentar el n√∫mero de repeticiones, esta estimaci√≥n se estabiliza. Podemos observar este fen√≥meno en la **Figura 3.**
```

![Convergencia de la frecuencia relativa](../images/frecuencia_relativa_frecuentista.png)

**Figura 3**. En el enfoque frecuentista, la probabilidad se interpreta como el valor al que tiende la _frecuencia relativa_ de un evento (por ejemplo, "cara" en una moneda) conforme se incrementa el n√∫mero de repeticiones del experimento.

```{admonition} ¬øEs s√≥lo una f√≥rmula?
:class: caution

Aunque el enfoque frecuentista utiliza la _frecuencia relativa_ para estimar la probabilidad, **no se reduce solamente a una f√≥rmula**.

Este enfoque implica una forma espec√≠fica de entender qu√© es una probabilidad:

1. La probabilidad de un evento se **define** como el **l√≠mite** de su _frecuencia relativa_ al repetir el experimento muchas veces.

$$\lim_{n \to \infty} \frac{\#A}{n} = \mathbb{P}(A)$$

2. Se asume que las probabilidades son **propiedades objetivas del mundo**, no creencias subjetivas.
3. No se habla de probabilidades en eventos √∫nicos o no repetibles, como ‚Äúla probabilidad de que llueva ma√±ana‚Äù.

Por tanto, el frecuentismo es una postura **matem√°tica y filos√≥fica**, no solo un m√©todo de c√°lculo.
```

> **¬øQu√© significa "variable aleatoria" en estad√≠stica frecuentista?**

Hasta ahora, posiblemente has visto que una _variable aleatoria_ es algo que usamos para modelar resultados inciertos, como lanzar un dado, medir una altura, o contar cu√°ntas veces sale cara una moneda.

Y eso es, precisamente, lo que significa en el enfoque **frecuentista**.

> Una _variable aleatoria_ representa el **resultado num√©rico** de un experimento aleatorio que se puede repetir muchas veces.

Por ejemplo, si lanzas una moneda y defines $(X = 1)$ si sale cara y $(X = 0)$ si sale cruz, entonces $(X)$ es una _variable aleatoria_.

Aqu√≠, lo aleatorio es el **resultado** del experimento, **no** el par√°metro que describe su comportamiento (como la probabilidad de que salga cara).

Desde esta perspectiva, _los par√°metros son constantes fijas, pero desconocidas_ y no se modelan con variables aleatorias.

```{admonition} Fundamento te√≥rico
:class: note

El hecho de que la frecuencia relativa se estabilice a medida que aumenta el n√∫mero de repeticiones est√° respaldado por un resultado matem√°tico conocido como la [**ley de los grandes n√∫meros**](https://es.wikipedia.org/wiki/Ley_de_los_grandes_n%C3%BAmeros).
```

##### Enfoque bayesiano: probabilidades como creencias

_¬øQu√© significa decir que algo tiene cierta probabilidad de ocurrir desde la perspectiva bayesiana?_

El enfoque bayesiano propone una forma muy intuitiva de verlo:

> La probabilidad es una medida de qu√© tanto creemos que algo es cierto, bas√°ndonos en lo que sabemos hasta ahora.

As√≠, para los bayesianos, la probabilidad no es una propiedad fija del mundo como ‚Äúla gravedad‚Äù, sino m√°s bien una forma de representar nuestra incertidumbre. Y lo m√°s importante:

> üí° Esa creencia puede cambiar si recibimos nueva informaci√≥n.

```{admonition} Ejemplo
Sup√≥n que alguien te dice:

> _"Hay probabilidad de lluvia ma√±ana"._

Esa probabilidad puede ser diferente dependiento de si:

* viste un pron√≥stico confiable,

* el cielo est√° completamente despejado,

* o escuchaste truenos a lo lejos.

Cada nueva pista _cambia tu creencia_. Y esto es justamente lo que el enfoque bayesiano busca formalizar: **c√≥mo actualizamos** nuestras creencias cuando obtenemos nueva evidencia.
```

> ¬øC√≥mo se hace esto?

La herramienta central para actualizar nuestras creencias es la **probabilidad condicional**:

$$
\mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$

Esta f√≥rmula expresa la probabilidad de que ocurra un evento $(A)$, dado que se ha observado otro evento $(B)$. La **probabilidad condicional** es el primer paso hacia un modelo bayesiano completo, ya que introduce la noci√≥n de **informaci√≥n que modifica creencias**.

```{admonition} Probabilidad condicional
:class: tip

La probabilidad condicional nos dice cu√°l es la probabilidad de que ocurra un evento $(A)$, dado que ya sabemos que ha ocurrido otro evento $(B)$.

![](../images/sesion2-probcondi.png)

Por ejemplo, la probabilidad de lluvia puede cambiar si sabemos que el cielo est√° despejado. Esta forma de actualizar creencias es esencial en el enfoque **bayesiano**.
```

Aqu√≠ es donde entra en escena el famoso **Teorema de Bayes**.

```{admonition} Teorema de Bayes: actualizaci√≥n de creencias
:class: important

El Teorema de Bayes permite **actualizar una creencia previa** cuando se incorpora nueva informaci√≥n. Se compone de tres elementos:

- **Prior**: lo que cre√≠amos antes de observar datos $(P(A))$
- **Likelihood** (verosimilitud): qu√© tan probable es observar los datos si la hip√≥tesis fuera cierta $(P(B \mid A))$
- **Posterior**: lo que creemos **despu√©s** de observar los datos $(P(A \mid B))$

![](../images/sesion2-probcondi2.png)

Este resultado es matem√°ticamente demostrable, y es la base de toda la estad√≠stica bayesiana.
```

> **¬øQu√© significa "variable aleatoria" en estad√≠stica bayesiana??**

Hasta ahora, posiblemente has visto que una variable aleatoria es algo que usamos para modelar resultados inciertos, como lanzar un dado, medir una altura, o contar cu√°ntas veces sale cara una moneda.

Pero en el mundo bayesiano, las cosas se vuelven m√°s interesantes:

> Tambi√©n usamos variables aleatorias para representar lo que no sabemos sobre un par√°metro.

Por ejemplo, imagina que est√°s tratando de averiguar cu√°l es la probabilidad $\theta$ de que una m√°quina falle en un d√≠a cualquiera.
No tienes un n√∫mero exacto, pero...

> _Creo que la m√°quina es bastante confiable, as√≠ que probablemente $\theta$ sea algo as√≠ como 0.1... pero no estoy segura"_

Aqu√≠ no est√°s hablando de un resultado aleatorio, como el lanzamiento de un dado. Est√°s hablando de tu **incertidumbre sobre un valor desconocido.**

Y eso, en bayes, se modela con una _variable aleatoria._

> **¬øY c√≥mo describimos esa incertidumbre?**

Usamos lo que se llama una **distribuci√≥n de probabilidad**, que nos dice qu√© tan probable o cre√≠ble creemos que es cada posible valor de $\theta$.

Por ejemplo:

- Si pensamos que valores peque√±os de $\theta$ (como 0.05 o 0.1) son m√°s probables, la distribuci√≥n ser√° m√°s alta en esa zona.

- Si valores como 0.8 o 0.9 nos parecen muy poco probables, la distribuci√≥n ser√° muy baja all√≠.

Esto se representa con una funci√≥n como:

$$
p_X(x)
$$

significa que:

- $(X)$: es la variable aleatoria que representa la probabilidad de un evento (como que salga cara).
- $(x)$: es un valor posible de esa probabilidad (por ejemplo, 0.7).
- $(p_X(x))$: es cu√°n cre√≠ble o probable consideramos ese valor, dada la informaci√≥n disponible.

![](../images/sesion2-bayes_prob_de_probabilidad.png)

**Figura 4.** Notaci√≥n de la funci√≥n de densidad de probabilidad: $p_X(x)$ represeta cu√°n probable o cre√≠ble creemos que es cada posible valor del par√°metro X.

En lugar de afirmar ‚Äúla probabilidad es 0.7‚Äù, el bayesiano dice: ‚Äú0.7 es plausible, pero tambi√©n lo son otros valores cercanos y esta es la distribuci√≥n que los describe‚Äù.

![](../images/bayes_prior_posterior.png)

**Figura 5.** En el enfoque bayesiano, una creencia inicial (distribuci√≥n prior, l√≠nea azul) se combina con la evidencia aportada por los datos (verosimilitud, l√≠nea roja) para producir una creencia actualizada (posterior, l√≠nea verde). Esto se realiza aplicando el Teorema de Bayes.

```{admonition} Resumen
:class: tip

En estad√≠stica bayesiana:
- Los par√°metros desconocidos se tratan como variables aleatorias.

- Modelamos nuestras creencias sobre esos valores con distribuciones de probabilidad.

- Cuando obtenemos nuevos datos, actualizamos esas creencias usando el Teorema de Bayes.
```

<iframe width="650" height="315"
    src="https://www.youtube.com/embed/XIbL0foEckA"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen>
</iframe>

---

![](../images/memes/interpretations-joke.png)

---

Una vez que entendemos qu√© representa una probabilidad y c√≥mo se puede interpretar, el siguiente paso natural es analizar **c√≥mo se combinan o se relacionan distintos eventos entre s√≠**.

Esto nos lleva a estudiar conceptos fundamentales como la **probabilidad condicional, conjunta y marginal**.

#### 1.2.7. Probabilidad conjunta, condicional y marginal

Hasta ahora hemos aprendido a:

- Definir un **espacio muestral** $(\Omega)$, que contiene todos los posibles resultados de un experimento.
- Considerar subconjuntos de $(\Omega)$ como **eventos**, organizados en una estructura llamada $(\sigma)$-√°lgebra.
- Asignar valores de **probabilidad** a esos eventos mediante funciones que cumplen ciertos principios (como los axiomas de Kolmog√≥rov).
- Interpretar esos valores bajo los enfoques **frecuentista** y **bayesiano**.

Este marco nos permite describir eventos **individuales**, pero tambi√©n **eventos compuestos**: por ejemplo, cuando dos condiciones ocurren a la vez, o una depende de otra.

---

##### ¬øQu√© nos permite considerar eventos compuestos?

La clave est√° en la $(\sigma)$-√°lgebra:

> Al definir el espacio de eventos como una $(\sigma)$-√°lgebra, garantizamos que podemos operar con ellos de forma l√≥gica y estructurada:
>
> - unir eventos $(A \cup B)$
> - tomar complementos $(A^c)$
> - y tambi√©n **intersecciones** como $(A \cap B)$, que representan **eventos conjuntos**.

Esto no solo nos permite construir descripciones m√°s ricas de situaciones, sino que nos habilita a hacer **operaciones de probabilidad** sobre m√∫ltiples eventos.

A continuaci√≥n, exploraremos tres operaciones fundamentales:

---

##### 1. Probabilidad conjunta

La **probabilidad conjunta** mide la probabilidad de que **dos eventos ocurran al mismo tiempo**.

Se denota como:

$$
\mathbb{P}(A \cap B)
$$

o, si el contexto lo permite, simplemente $\mathbb{P}(A, B)$.

```{admonition} Ejemplo
:class: tip

Si $(A = \{\text{llover}\})$ y $(B = \{\text{llevar paraguas}\})$,
entonces $(\mathbb{P}(A \cap B))$ representa la probabilidad de que **llueva y lleves paraguas** el mismo d√≠a.
```

##### 2. Probabilidad condicional

La **probabilidad condicional** responde a la pregunta:

> ¬øCu√°l es la probabilidad de que ocurra \(A\) si ya sabemos que ocurri√≥ \(B\)?

Se define como:

$$
\mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \quad \text{si } \mathbb{P}(B) > 0
$$

```{admonition} Ejemplo
:class: tip

Si $(\mathbb{P}(\text{llover y llevar paraguas}) = 0.3 )$
y $(\mathbb{P}(\text{llevar paraguas}) = 0.5)$,
entonces:

$$
\mathbb{P}(\text{llover} \mid \text{llevar paraguas}) = \frac{0.3}{0.5} = 0.6
$$
```

##### 3. Probabilidad marginal

La **probabilidad marginal** es la probabilidad de un evento **sin condicionar** en ning√∫n otro.

Cuando trabajamos con variables m√∫ltiples, podemos obtener la marginal de una variable ‚Äúresumiendo‚Äù la informaci√≥n sobre las otras.

$$
\mathbb{P}(A) = \sum_{b} \mathbb{P}(A, B=b)
$$

En el caso continuo:

$$
\mathbb{P}(A) = \int \mathbb{P}(A, B)\, dB
$$

```{admonition} Nota
:class: note

La marginal es √∫til cuando queremos concentrarnos en un solo evento o variable, ignorando otras que tambi√©n est√°n en juego.
```

#### 1.2.8. Reglas derivadas de la probabilidad

Una vez que hemos definido formalmente la **probabilidad condicional**, podemos deducir tres reglas fundamentales que se utilizan constantemente en modelado probabil√≠stico.

Estas reglas no son nuevos axiomas, sino **consecuencias directas** de combinar:

- Los **axiomas de Kolmog√≥rov**, especialmente:

  - La aditividad: $P(A \cup B) = P(A) + P(B)$ si $A \cap B = \varnothing$

- La **definici√≥n de probabilidad condicional**

Veamos cada una:

##### üìå Regla de marginalizaci√≥n

Se basa en la **aditividad**: si $B$ puede tomar varios valores disjuntos, entonces:

$$
P(A) = \sum_B P(A, B)
$$

Esta regla permite **reducir** una probabilidad conjunta a una **marginal**.

---

##### üìå Regla de la cadena

Viene directamente de _reordenar_ la definici√≥n de probabilidad condicional:

$$
P(A \mid B) = \frac{P(A, B)}{P(B)} \quad \Rightarrow \quad P(A, B) = P(A \mid B) \cdot P(B)
$$

Esta relaci√≥n es fundamental en estructuras secuenciales como los modelos gr√°ficos.

Tambi√©n puede escribirse sim√©tricamente:

$$
P(B, A) = P(B \mid A) \cdot P(A)
$$

---

##### üìå Regla de la probabilidad total

Surge al aplicar **marginalizaci√≥n** sobre la **regla de la cadena**:

$$
P(A) = \sum_B P(A, B) = \sum_B P(A \mid B) \cdot P(B)
$$

Es decir, descomponemos la probabilidad de $A$ en funci√≥n de sus componentes condicionales respecto a $B$.

```{admonition} Nota t√©cnica
:class: tip

En el contexto de probabilidad, usamos la notaci√≥n $P(A, B)$ como abreviatura de $P(A \cap B)$ (la probabilidad de que ocurran ambos eventos).
```

> Adem√°s de ser √∫til para calcular probabilidades marginales,  
> la **regla de la probabilidad total** tambi√©n puede interpretarse como una **constante de normalizaci√≥n**:
>
> garantiza que la distribuci√≥n condicional resultante \( P(A \mid B) \) sea **v√°lida**,  
> es decir, que **sume 1** al considerar todos los posibles valores de \(A\).

```{math}
\sum_A P(A \mid B) = 1
```

![](../images/reglas_probabilidad_diagrama.png)

**Figura 5.** La definici√≥n de probabilidad condicional permite derivar tres reglas fundamentales: la **regla de la cadena**, la **regla de marginalizaci√≥n** y la **regla de la probabilidad total**. Estas relaciones son la base del razonamiento probabil√≠stico en contextos donde intervienen m√∫ltiples eventos.

#### 1.2.9. Independencia

Uno de los conceptos m√°s importantes en teor√≠a de probabilidad es el de **independencia entre eventos o variables**. Intuitivamente, dos eventos son independientes si el conocimiento de uno **no cambia** la probabilidad del otro.

> La independencia **no se asume** ni se deduce por intuici√≥n: **se verifica formalmente**.

```{admonition} Definici√≥n formal
:class: tip

Dos eventos $A$ y $B$ son independientes si:

$$
\mathbb{P}(A \mid B) = \mathbb{P}(A)
\quad \text{o equivalentemente} \quad
\mathbb{P}(A \cap B) = \mathbb{P}(A) \cdot \mathbb{P}(B)
$$

Esta √∫ltima igualdad es m√°s com√∫nmente usada, ya que no requiere el c√°lculo de probabilidades condicionales (evita divisiones por cero cuando $\mathbb{P}(B) = 0$).
```

##### Simetr√≠a

Una propiedad importante de la independencia es que **es sim√©trica**:

Si $A$ es independiente de $B$, entonces $B$ es independiente de $A$.

##### Alternativa para variables

Dos variables aleatorias $X$ y $Y$ son independientes si:

$$
\mathbb{P}(X = x, Y = y) = \mathbb{P}(X = x) \cdot \mathbb{P}(Y = y)
\quad \text{para todo } x, y
$$

```{admotion} Nota
:class: note

**¬øC√≥mo lo verificamos?**

- Si las variables son **discretas**:
  Verifica la igualdad para **todas las combinaciones posibles** de valores.

- Si son **continuas**:
  Verifica si la densidad conjunta se puede escribir como el **producto de densidades marginales**.
```

> üìå _Nota:_ basta con que una sola combinaci√≥n no cumpla la igualdad para que las variables no sean independientes.

```{admonition} Ejemplo
:class: tip

Sup√≥n que lanzamos dos monedas. Sea $\alpha$: _‚Äúla primera moneda resulta en cara‚Äù_, y $\beta$: _‚Äúla segunda moneda resulta en cara‚Äù_. Estos eventos son independientes: saber que uno ocurri√≥ no afecta la probabilidad del otro.

Tambi√©n en procesos m√°s abstractos, la independencia puede surgir de la estructura del experimento. Por ejemplo:

- $\alpha$: _"el resultado del dado es par"_
- $\beta$: _"el resultado es 1 o 2"_

Si el dado es justo, es f√°cil verificar que:

$$
\mathbb{P}(\alpha \cap \beta) = \mathbb{P}(\alpha) \cdot \mathbb{P}(\beta) = \frac{1}{2} \cdot \frac{1}{3} = \frac{1}{6}
$$

Por lo tanto, $\alpha$ y $\beta$ son eventos independientes.
```

#### 1.2.10. Independencia condicional

La **independencia** es una propiedad √∫til, pero poco com√∫n en problemas reales:  
en la pr√°ctica, es raro que dos variables o eventos sean verdaderamente independientes.

Sin embargo, lo que s√≠ ocurre con frecuencia es que **dos variables sean independientes _condicionalmente_** a una tercera.  
Es decir:

> **Dado que sabemos algo**, esa informaci√≥n ‚Äúexplica‚Äù o ‚Äúdesvincula‚Äù a las otras dos variables.

```{admonition} Definici√≥n formal
:class: tip

Decimos que $A$ y $B$ son **condicionalmente independientes dado $X$**, si se cumple:

$$
\mathbb{P}(A, B \mid X) = \mathbb{P}(A \mid X) \cdot \mathbb{P}(B \mid X)
$$

Lo cual tambi√©n implica que:

$$
\mathbb{P}(A \mid B, X) = \mathbb{P}(A \mid X)
\quad \text{y} \quad
\mathbb{P}(B \mid A, X) = \mathbb{P}(B \mid X)
$$
```

> üìå _Nota:_ La informaci√≥n de $X$ ‚Äúbloquea‚Äù la dependencia entre $A$ y $B$.  
> Es decir, **una vez que sabemos $X$**, las otras dos variables > **no se influyen entre s√≠**.

```{admonition} Ejemplo
:class: tip

Sup√≥n que queremos estimar la probabilidad de que una estudiante sea admitida a un posgrado en el ITAM o en el CINVESTAV.

- Sea $I$: _"fue admitida al posgrado del ITAM"_
- Sea $C$: _"fue admitida al posgrado del CINVESTAV"_

Normalmente, $I$ y $C$ **no son independientes**: saber que fue admitida al ITAM probablemente aumenta nuestra creencia de que tambi√©n ser√° admitida al CINVESTAV, porque sugiere que es una estudiante sobresaliente.

Ahora supongamos que sabemos que su promedio de licenciatura ($G$) es de 9.5, y que ambas instituciones **basan su admisi√≥n √∫nicamente en el promedio**.

Entonces, **dado $G = 9.5$**, los eventos $I$ y $C$ s√≠ son independientes:

$$
\mathbb{P}(C \mid I, G) = \mathbb{P}(C \mid G)
$$

En este caso, decimos que **$C$ es condicionalmente independiente de $I$ dado $G$**.
```

```{admonition} Tip
Diremos que $A \perp B \mid C$ si se cumple:

$$
\mathbb{P}(A \cap B \mid C) = \mathbb{P}(A \mid C) \cdot \mathbb{P}(B \mid C)
$$
```

```{admonition} Notaci√≥n com√∫n

$A \perp B \mid C$:
A es independiente de B dado C.

Tambi√©n se puede escribir:
$\mathbb{P}(A \mid B, C) = \mathbb{P}(A \mid C)$
```
