{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caecfa61",
   "metadata": {},
   "source": [
    "## Sesión 4\n",
    "\n",
    "## Estimadores de máxima verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bac9bf",
   "metadata": {},
   "source": [
    "```{figure} ../images/sesion4-intro.png\n",
    ":alt: representacion\n",
    ":fig-align: center\n",
    ":width: 600px\n",
    "\n",
    "Conceptos clave de la sesión 4.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9bbe8",
   "metadata": {},
   "source": [
    "> **Objetivos de la sesión:**\n",
    "> - Comprender el principio de máxima verosimilitud a través de ejemplos básicos.\n",
    "> - Estimar los parámetros de algunas distribuciones comunes usando el principio de máxima verosimilitud.\n",
    "> - Entender las limitaciones básicas de los estimadores de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc0dd1",
   "metadata": {},
   "source": [
    "### Modelos probabilísticos\n",
    "\n",
    "Un _modelo probabilístico_ es una especificación de la **distribución conjunta de las variables aleatorias** involucradas en el problema, condicionada (o no) por un conjunto de **párametros desconocidos.**\n",
    "\n",
    "$$\n",
    "\\text{Modelo:} \\quad P(X_1, X_2, \\dots, X_n \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $X_1, \\dots, X_n$ son las variables aleatorias (datos observados).\n",
    "\n",
    "- $\\theta \\in \\Theta$ es el vector de parámetros desconocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af73421",
   "metadata": {},
   "source": [
    "Muy comunmente mi modelo dependerá de _parámetros desconocidos_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38db16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34cd6b5",
   "metadata": {},
   "source": [
    "Veamos brevemente la diferencia entre *parámetros* e *hiperparámetros*:\n",
    "\n",
    "|                         | **Parámetros**    | **Hiperparámetros**                                                              |\n",
    "|-------------------------|-------------------|----------------------------------------------------------------------------------|\n",
    "| **Definición**          | Valores internos aprendidos automáticamente por el modelo durante el entrenamiento. | Valores externos definidos antes del entrenamiento que controlan el proceso de aprendizaje. |\n",
    "| **Ejemplos**            | Pesos, sesgos, coeficientes de una red neuronal, vectores de embeddings.        | Tasa de aprendizaje, número de épocas, número de capas, tamaño del batch, regularización. |\n",
    "| **Características**     | Se ajustan iterativamente con el algoritmo de optimización (ej. descenso de gradiente). | Se eligen manualmente o con técnicas de optimización de hiperparámetros (grid search u otros).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db09b18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf473d",
   "metadata": {},
   "source": [
    "Ahora que entendemos la diferencia entre parámetros y hiperparámetros, podemos volver al corazón del modelo probabilístico. Para ello, retomaremos algunos conceptos básicos de la teoría de probabilidad y los conectaremos con el principio de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248fc03",
   "metadata": {},
   "source": [
    "#### ¿Por qué necesitamos distribuciones de probabilidad?\n",
    "\n",
    "Cuando observamos datos del mundo real, rara vez son idénticos: siempre existe variabilidad. Esa variabilidad refleja la incertidumbre inherente al proceso que generó los datos.\n",
    "\n",
    "Para poder construir un modelo que explique cómo se producen esos datos, necesitamos suponer una estructura probabilística que capture esa incertidumbre.\n",
    "\n",
    "Aquí es donde entran en juego las **distribuciones de probabilidad**:\n",
    "\n",
    "* Nos permiten representar matemáticamente la incertidumbre presente en los datos.\n",
    "* Sirven para formalizar nuestras suposiciones sobre cómo se comporta el proceso generador de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411bd777",
   "metadata": {},
   "source": [
    "1. *Necesitamos una distribución de probabilidad*\n",
    "- Para describir cómo se comportan los datos.  \n",
    "- Esa distribución refleja nuestras **hipótesis** de comportamiento sobre el fenómeno (por ejemplo, normal, binomial, exponencial, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea7a76",
   "metadata": {},
   "source": [
    "2. *Elegir la forma de la distribución es un supuesto*\n",
    "- La persona quién construye el modelo decide qué distribución usar.  \n",
    "- Esto es una **hipótesis de modelado** y puede ser correcta, aproximada o equivocada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856807ea",
   "metadata": {},
   "source": [
    "```{figure} ../images/comparacion_distribuciones_ajustadas.png\n",
    ":alt: representacion\n",
    ":fig-align: center\n",
    ":width: 600px\n",
    "\n",
    "Comparación de distribuciones ajustadas a los datos.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dff02f",
   "metadata": {},
   "source": [
    "3. *Distribuciones tienen parámetros*\n",
    "- Cada familia de distribuciones está definida por **parámetros desconocidos**:  \n",
    "  - Normal → media $\\mu$ y varianza $\\sigma^2$.  \n",
    "  - Poisson → tasa $\\lambda$.  \n",
    "  - Bernoulli → probabilidad de éxito $p$.  \n",
    "- Al no conocerlos, tratamos de *estimarlos a partir de datos* (máxima verosimilitud, métodos bayesianos, etc.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ca93e",
   "metadata": {},
   "source": [
    "```{admonition} Parámetros conocidos\n",
    ":class: tip\n",
    "Los parámetros son desconocidos en la mayoría de los casos, pero se pueden considerar conocidos cuando:\n",
    "\n",
    "* Los fijamos en un modelo o simulación.\n",
    "\n",
    "* La teoría/experimento nos da su valor exacto.\n",
    "\n",
    "* Los tratamos como supuestos para simplificar.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27f2c6",
   "metadata": {},
   "source": [
    "4. *Familia de distribuciones posibles*\n",
    "- Antes de ver los datos, no sabemos qué valores toman los parámetros.  \n",
    "- Así, realmente no hablamos de “una sola distribución”, sino de una *familia de distribuciones* parametrizadas.  \n",
    "- El trabajo de la estadística/inferencia es reducir la incertidumbre sobre los parámetros → elegir la distribución concreta que mejor describe el comportamiento de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f978fcf",
   "metadata": {},
   "source": [
    "```{figure} ../images/familia_distribuciones_gamma.png\n",
    ":alt: representacion\n",
    ":fig-align: center\n",
    ":width: 600px\n",
    "\n",
    "Familia de distribuciones Gamma.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39e9bd",
   "metadata": {},
   "source": [
    "En suma, tu modelo probabilístico empieza como una *familia de distribuciones parametrizadas*.  \n",
    "- La elección de la familia es un *supuesto tuyo*.  \n",
    "- Los parámetros son *desconocidos* y deben ser *estimados* o *aprendidos*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a340b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428d9e9",
   "metadata": {},
   "source": [
    "## Métodos de estimación o inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9f8cd",
   "metadata": {},
   "source": [
    "### (a) La estimación por máxima verosimilitud *(Maximum likelihood estimation)*\n",
    "\n",
    "Estima los parámetros $\\theta$ de un modelo probabilístico.\n",
    "\n",
    "* La idea básica es elegir los parámetros que _maximizan_ la **función de verosimilitud**.\n",
    "\n",
    "* Intuitivamente, esto corresponde a elegir los parámetros que _maximizan_ la probabilidad de los datos obsevados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5758e0",
   "metadata": {},
   "source": [
    "Inevitablemente comenzamos a hablar, entonces, de **estadística**... específicamente de **inferencia estadística**: Cuando tenemos datos observados y sacamos conclusiones a partir de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7953f1b",
   "metadata": {},
   "source": [
    "## Ejercicio: Máxima verosimilitud para distribuciones discretas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d40b10",
   "metadata": {},
   "source": [
    "De clases pasadas, sabemos que el experimento de tirar la moneda $n$ veces y contar el número de caras, sigue una distribución $\\text{Binomial}(n,\\theta)$, con **PMF**:\n",
    "\n",
    "$$\n",
    "p(a) = \\left(\\begin{array}{c}n \\\\ a \\end{array}\\right) \\theta^a (1 - \\theta)^{n-a}\n",
    "$$\n",
    "\n",
    "donde $a$ es el número de caras.\n",
    "\n",
    "En un ejemplo concreto, supongamos que se tiró la moneda 100 veces y contamos 55 caras. Por tanto, sabemos que\n",
    "\n",
    "$$\n",
    "p(55) = \\left(\\begin{array}{c}100 \\\\ 55\\end{array}\\right) \\theta^{55}(1 - \\theta)^{45}\n",
    "$$\n",
    "\n",
    "Observamos que la probabilidad de obtener 55 caras depende del valor de $\\theta$, por lo que es usual incluir esto con la notación de probabilidad condicional:\n",
    "\n",
    "$$\n",
    "p(55 | \\theta) = \\left(\\begin{array}{c}100 \\\\ 55\\end{array}\\right) \\theta^{55}(1 - \\theta)^{45}\n",
    "$$\n",
    "\n",
    "Lo anterior, lo podemos leer como: \"la probabilidad de obtener 55 caras dado que la probabilidad de cara en un tiro individual es $\\theta$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a36e64",
   "metadata": {},
   "source": [
    "Algunos términos convencionales que usamos en estadística:\n",
    "\n",
    "- **Experimento**: Tirar la moneda 100 veces y contar el número de caras.\n",
    "\n",
    "- **Datos**: Los datos son el resultado del experimento. En este caso son las 55 caras.\n",
    "\n",
    "- **Parámetros de interés**: Estamos interesados en conocer el valor del parámetro $\\theta$.\n",
    "\n",
    "- **Función de verosimilitud**: Es la función $p(datos | parámetros)$. Notemos que es una función tanto de los datos, como de los parámetros. En nuestro caso es\n",
    "  $$\n",
    "  p(55 | \\theta) = \\left(\\begin{array}{c}100 \\\\ 55\\end{array}\\right) \\theta^{55}(1 - \\theta)^{45}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d1025",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Descubrir</summary>\n",
    "\n",
    "Haciendo uso del cálculo, obtenemos lo siguiente:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} p(55 | \\theta) = \\left(\\begin{array}{c}100 \\\\ 55\\end{array}\\right) \\left(55\\theta^{54}(1 - \\theta)^{45} - 45\\theta^{55}(1 - \\theta)^{44}\\right)\n",
    "$$\n",
    "\n",
    "Igualando a cero:\n",
    "\n",
    "$$\n",
    "\\begin{align} \\nonumber\n",
    "55\\theta^{54}(1 - \\theta)^{45} = 45\\theta^{55}(1 - \\theta)^{44} \\\\ \\nonumber\n",
    "55(1 - \\theta) = 45\\theta \\\\ \\nonumber\n",
    "55 = 100 \\theta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por lo que el MLE es $\\hat{\\theta} = \\frac{55}{100}$.\n",
    "\n",
    "En los extremos ($\\theta=0$ o $\\theta=1$), la verosimilitud es nula, por lo que $\\hat{\\theta}=0.55$ es el único máximo en $[0,1]$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50398b0f",
   "metadata": {},
   "source": [
    "```{thebe-button}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfd882",
   "metadata": {
    "tags": [
     "Thebe"
    ]
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#from matplotlib import pyplot as plt\n",
    "#import math\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00cc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos la función de verosimilitud con los datos del ejemplo\n",
    "n = 100\n",
    "a = 55\n",
    "theta = np.linspace(0, 1, 1001)\n",
    "\n",
    "# Función de verosimilitud (evaluada)\n",
    "L = math.comb(n, a) * theta**a * (1-theta)**(n-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f489b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L --> es un vector de verosimilitudes para cada posible valor de theta\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa69288",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aproximar el valor máximo de la verosimilitud\n",
    "theta_mle1 = theta[np.argmax(L)]\n",
    "theta_mle1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35898c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafiquemos la función de verosimilitud\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(theta, \n",
    "         L, \n",
    "         label=\"$L(θ)$ Verosimilitud\")\n",
    "plt.axvline(x=theta_mle1, \n",
    "            color=\"red\", linestyle=\"--\", \n",
    "            label=fr\"$\\hat{{\\theta}}_{{MLE}} = {theta_mle1:.2f}$\")\n",
    "plt.title(\"Función de verosimilitud (Binomial)\")\n",
    "plt.xlabel(\"θ (probabilidad de éxito)\")\n",
    "plt.ylabel(\"$L(θ)$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf1930",
   "metadata": {},
   "source": [
    "> ¿qué pasa en el caso en donde se aumente la cantidad de muestras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28426ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de nuevo, pero con más datos:\n",
    "n_ = 1000\n",
    "a_ = 550\n",
    "\n",
    "theta_ = np.linspace(0, 1, 1001)\n",
    "L_ = math.comb(n_, a_) * theta_**a_ * (1-theta_)**(n_-a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3679d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aproximar el valor máximo de la verosimilitud\n",
    "theta_mle2 = theta[np.argmax(L_)]\n",
    "theta_mle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeadb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos las funcines de verosimilitud\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(theta, L, label='Verosimilitud - 100 tiros - 55 caras')\n",
    "plt.axvline(theta[np.argmax(L)], color='red', linestyle='--')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$L(\\theta)$')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(theta_, L_, label='Verosimilitud - 1000 tiros - 550 caras')\n",
    "plt.axvline(theta_[np.argmax(L_)], color='red', linestyle='--')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$L(\\theta)$')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d68bed",
   "metadata": {},
   "source": [
    "Aunque el MLE en ambos casos es el mismo, porque\n",
    "\n",
    "$$\n",
    "\\frac{550}{1000} = \\frac{55}{100},\n",
    "$$\n",
    "\n",
    "observamos que la dispersión de la función de verosimilitud al rededor del máximo en el caso de más tiros es menor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447a65e",
   "metadata": {},
   "source": [
    "> ¿En cuál de los dos valores estimados $\\hat{\\theta}$ confías más?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6a208",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdf1c4",
   "metadata": {},
   "source": [
    "### Muestra observada\n",
    "\n",
    "Se denota por $\\mathcal{D} = \\{x_1, x_2, \\dots, x_n\\}$ o como $\\mathcal{X}$. Es el conjunto de datos observados.\n",
    "\n",
    "#### Supuesto _i.i.d._\n",
    "\n",
    "Cuando trabajamos con una muestra $X_1, \\dots, X_n$, comúnmente **asumimos** que los datos son:\n",
    "\n",
    "- **Independientes**: El conocimiento de un valor no da información sobre otro.\n",
    "- **Idénticamente distribuidos**: Todos siguen la misma distribución $f(x; \\theta)$\n",
    "\n",
    "Esto se denota como:\n",
    "\n",
    "$$\n",
    "X_1, \\dots, X_n \\overset{\\text{i.i.d.}}{\\sim} f(x; \\theta)\n",
    "$$\n",
    "\n",
    "Estas suposiciones de _independencia_ lo que hacen es que los resultados del experimento se puedan **multiplicar** y esto, entonces es válido en cualquier contexto donde supongamos _independencia_ de los resultados del experimento. Esto permite factorizarla como:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} f(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $L(\\theta)$ es la **función de verosimilitud**\n",
    "- $x_i$ son las observaciones de la muestra\n",
    "- $\\theta$ es el parámetro del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409a19b",
   "metadata": {},
   "source": [
    "Con la suposición de que los datos son independientes e idénticamente distribuidos _(i.i.d.)_, trae como consecuencia que la función de verosimilitud sea un **producto** de las verosimilitudes individuales de los datos. Ahora, dado que estamos interesad@s en maximizar la verosimilitud, hay que tener en cuenta que maximizar productos de funciones puede tornarse bastante complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084b29f",
   "metadata": {},
   "source": [
    "### Log-verosimilitud\n",
    "\n",
    "Es en este punto donde nos podemos dar cuenta que la función $\\log$ (logaritmo natural o logaritmo en base $e$) puede ser de gran ayuda, dado que convierte productos en sumas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4d469",
   "metadata": {},
   "source": [
    "Usualmente, después de establecer la **multiplicatoria** de la verosimilitud, se aplica el **logaritmo natural** para simplificar el cálculo.  \n",
    "\n",
    "Esto transforma productos en sumas y hace más manejable el trabajo analítico:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\log L(\\theta) \n",
    "= \\log \\Bigg( \\prod_{i=1}^n f(x_i \\mid \\theta) \\Bigg) \n",
    "= \\sum_{i=1}^n \\log f(x_i \\mid \\theta).\n",
    "$$\n",
    "\n",
    "A esta función se le conoce como **log-verosimilitud**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d095d2",
   "metadata": {},
   "source": [
    "**El logaritmo es estrictamente creciente en $(0, \\infty)$**\n",
    "\n",
    "Esto significa que:\n",
    "\n",
    "$$\n",
    "x_1 < x_2 \\;\\;\\Rightarrow\\;\\; \\log(x_1) < \\log(x_2)\n",
    "$$\n",
    "\n",
    "Es decir, el logaritmo **preserva el orden** de los valores positivos.  \n",
    "\n",
    "Ejemplo: \n",
    "\n",
    "$$5 > 2 \\;\\;\\implies\\;\\; \\log(5) > \\log(2)$$\n",
    "\n",
    "En consecuencia, el valor de $\\theta$ que maximiza la verosimilitud $L(\\theta)$ es el mismo que maximiza la log-verosimilitud $\\ell(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07193ae8",
   "metadata": {},
   "source": [
    "**Ejemplo.** Retomando el caso de la moneda, tenemos que la log-verosimilitud es:\n",
    "\n",
    "$$\n",
    "\\log p(55 | \\theta) = \\log \\left(\\left(\\begin{array}{c}100 \\\\ 55\\end{array}\\right)\\right) + 55 \\log \\theta + 45 \\log (1 - \\theta)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b8a2d",
   "metadata": {},
   "source": [
    "Ahora, para maximizar la log-verosimilitud:\n",
    "\n",
    "<details>\n",
    "<summary>Descubrir</summary>\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} \\log p(55 | \\theta) = \\frac{55}{\\theta} - \\frac{45}{1 - \\theta}\n",
    "$$\n",
    "\n",
    "Igualando a cero:\n",
    "\n",
    "$$\n",
    "\\begin{align} \\nonumber\n",
    "\\frac{55}{\\theta} - \\frac{45}{1 - \\theta} \\\\ \\nonumber\n",
    "55(1 - \\theta) = 45\\theta \\\\ \\nonumber\n",
    "55 = 100 \\theta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Por lo que el MLE es $\\hat{\\theta} = \\frac{55}{100}$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832b441",
   "metadata": {},
   "source": [
    "Veamos como luce la función de log-verosimilitud en este caso y comparémosla con la función de verosimilitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e775d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de log-verosimilitud\n",
    "log_likelihood = np.log(float(math.comb(n, a))) + a * np.log(theta) + (n - a) * np.log(1 - theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1fd8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aproximar el valor máximo de la verosimilitud\n",
    "theta[np.argmax(log_likelihood)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d326d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos las funciones de log-verosimilitud y verosimilitud\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(theta, \n",
    "         L, label='Verosimilitud - 100 tiros - 55 caras')\n",
    "plt.axvline(theta[np.argmax(L)], color='red', linestyle='--')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$L(\\theta)$')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(theta, \n",
    "         log_likelihood, label='log-Verosimilitud - 100 tiros - 55 caras')\n",
    "plt.axvline(theta[np.argmax(log_likelihood)], color='red', linestyle='--')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$\\log L(\\theta)$')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682bc4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fedee6",
   "metadata": {},
   "source": [
    "## Ejercicio: Máxima verosimilitud para distribuciones continuas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ab993",
   "metadata": {},
   "source": [
    "```{admonition} Todo aplica a distribuciones continuas\n",
    ":class: hint\n",
    "\n",
    "En el caso que acabamos de analizar teníamos una distribución discreta $\\text{Binomial}$, y obtuvimos la función de verosimilitud usando la **PMF**. Para distribuciones continuas, todo lo que vimos es completamente aplicable, solo que usaremos la **PDF**.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bb99e",
   "metadata": {},
   "source": [
    "Suponemos que el tiempo de vida de los bombillos es modelado por una distribución exponencial con parámetro desconocido $\\lambda$. Probamos 5 bombillos, y observamos que tienen tiempos de vida de 2, 3, 1, 3, y 4 años, respectivamente. ¿Cuál es el MLE para $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3872b",
   "metadata": {},
   "source": [
    "Distribución exponencial:\n",
    "\n",
    "$$\n",
    "f(x; \\lambda) = \\lambda e^{-\\lambda x} \\quad \\text{para } x \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dceaad0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Descubrir</summary>\n",
    "\n",
    "Asumiendo i.i.d., tenemos que la densidad conjunta es:\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, x_3, x_4, x_5 | \\lambda) = p(x_1 | \\lambda) p(x_2 | \\lambda) p(x_3 | \\lambda) p(x_4 | \\lambda) p(x_5 | \\lambda) = (\\lambda e^{-\\lambda x_1}) (\\lambda e^{-\\lambda x_2}) (\\lambda e^{-\\lambda x_3}) (\\lambda e^{-\\lambda x_4}) (\\lambda e^{-\\lambda x_5}) = \\lambda^5 e^{-\\lambda(x_1 + x_2 + x_3 + x_4 + x_5)}\n",
    "$$\n",
    "\n",
    "Ahora, viendo los datos como fijos, con $x_1=2$, $x_2=3$, $x_3=1$, $x_4=3$, y $x_5=4$, y $\\lambda$ como variable, obtenemos la función de verosimilitud:\n",
    "\n",
    "$$\n",
    "L(\\lambda) = p(2, 3, 1, 3, 4 | \\lambda) = \\lambda^5 e^{-13\\lambda},\n",
    "$$\n",
    "\n",
    "y la log-verosimilitud:\n",
    "\n",
    "$$\n",
    "\\log L(\\lambda) = 5 \\log \\lambda - 13 \\lambda.\n",
    "$$\n",
    "\n",
    "Finalmente, usamos cálculo para encontrar el MLE:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d \\lambda} \\log L(\\lambda) = \\frac{5}{\\lambda} - 13\n",
    "$$\n",
    "\n",
    "Igualando a cero y despejando, obtenemos que el MLE es $\\hat{\\lambda} = \\frac{5}{13}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d9d65",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Descubrir</summary>\n",
    "\n",
    "Una vez hallado $\\hat{\\lambda} = \\frac{5}{13}$, podemos construir la distribución estimada:\n",
    "\n",
    "$$\n",
    "f(x; \\hat{\\lambda}) = \\frac{5}{13} e^{- \\frac{5}{13} x}\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caf9fa",
   "metadata": {},
   "source": [
    "```{admonition} En términos sencillos\n",
    "\n",
    ":class: note\n",
    "\n",
    "- Antes tenías una *familia de distribuciones exponenciales*, una para cada posible valor de $\\lambda$.\n",
    "- Ahora tienes *una única distribución estimada* con $\\hat{\\lambda} = \\frac{5}{13}$.\n",
    "\n",
    "**¿Qué se puede hacer con esta distribución estimada?**\n",
    "\n",
    "Con:\n",
    "\n",
    "$$\n",
    "f(x; \\hat{\\lambda}) = \\frac{5}{13} e^{-\\frac{5}{13}x}\n",
    "$$\n",
    "\n",
    "puedes:\n",
    "\n",
    "- **Calcular probabilidades**, por ejemplo:\n",
    "\n",
    "  $$\n",
    "  P(X < 2) = \\int_0^2 f(x; \\hat{\\lambda})\\, dx\n",
    "  $$\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6747a22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff937e",
   "metadata": {},
   "source": [
    "Veamos ahora un ejemplo práctico que muestra cómo el **estimador de máxima verosimilitud (MLE)** puede utilizarse para resolver un problema de probabilidad.  \n",
    "\n",
    "Partimos del modelo exponencial ajustado con $\\hat{\\lambda} = \\tfrac{5}{13}$ y queremos calcular la probabilidad de que un bombillo dure **menos de 2 años**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5911941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x', positive=True)\n",
    "lmbda = 5/13\n",
    "\n",
    "# PDF\n",
    "f = lmbda * sp.exp(-lmbda*x)\n",
    "\n",
    "# Integral\n",
    "P = sp.integrate(f, (x, 0, 2))\n",
    "P_eval = float(P)\n",
    "\n",
    "P_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077d7d3",
   "metadata": {},
   "source": [
    "- Suposición: Los *tiempos de vida de los bombillos* siguen una **Exponencial($\\lambda$)**.  \n",
    "- Datos observados: $2, 3, 1, 3, 4$.  \n",
    "- Con MLE obtuvimos que el parámetro es  \n",
    "\n",
    "$$\n",
    "\\hat\\lambda = \\frac{5}{13} \\approx 0.3846.\n",
    "$$\n",
    "\n",
    "Entonces, nuestro **modelo estimado** es:\n",
    "\n",
    "$$\n",
    "f(x;\\hat\\lambda) = 0.3846 \\, e^{-0.3846 x}, \\quad x \\geq 0.\n",
    "$$\n",
    "\n",
    "El cálculo fue:\n",
    "\n",
    "$$\n",
    "P(X < 2) = \\int_0^2 0.3846 \\, e^{-0.3846 x}\\,dx \\approx 0.537.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474569f8",
   "metadata": {},
   "source": [
    "- *Según el modelo exponencial estimado*, la probabilidad de que un bombillo dure menos de 2 años es aproximadamente 53.7%.  \n",
    "- Es decir, más de la mitad de los bombillos producidos bajo este modelo fallarían antes de los 2 años. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccbe3a3",
   "metadata": {},
   "source": [
    "## Ejercicio: Máxima verosimilitud con más de un parámetro\n",
    "\n",
    "En este caso tenemos:\n",
    "\n",
    "- Variable aleatoria $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "\n",
    "- $p(x | \\mu,\\sigma^2)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0bf44",
   "metadata": {},
   "source": [
    "Los estimadores de máxima verosimilitud de $\\mu$ y $\\sigma$ son:\n",
    "\n",
    "$$\\hat{\\mu}_{MLE} = \\frac{1}{N} \\sum_{j=1}^{N}x_j \\qquad \\text{y} \\qquad \\hat{\\sigma}_{MLE} = \\sqrt{\\frac{1}{N}\\sum_{j=1}^{N}(x_j-\\hat{\\mu}_{MLE})^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b084a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Númericamente:\n",
    "#from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e788923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros reales de la dist. normal\n",
    "mu_true = 10\n",
    "sigma_true = 2\n",
    "X = stats.norm(loc=mu_true, scale=sigma_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos muestras\n",
    "n_samples = 100000\n",
    "samples = X.rvs(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de máxima versimilitud\n",
    "mu_mle = np.mean(samples)\n",
    "sigma_mle = np.sqrt(np.mean((samples - mu_mle)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos los parámetros reales con los de máxima verosimilitud\n",
    "mu_true, mu_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_true, sigma_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b02321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma más sencilla:\n",
    "#from scipy.stats import norm\n",
    "\n",
    "# Ajustar distribución normal con MLE\n",
    "mu_mle, sigma_mle = norm.fit(samples)\n",
    "\n",
    "print(f\"Parámetros estimados por MLE: mu = {mu_mle}, sigma = {sigma_mle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf2c2c",
   "metadata": {},
   "source": [
    "## Comentarios finales\n",
    "\n",
    "El principio de máxima verosimilitud es bastante poderoso, y además una técnica general para estimar los parámetros de un modelo probabilístico. \n",
    "\n",
    " * _Overfitting_\n",
    "\n",
    "Sin embargo, tiene un problema: **en caso de tener pocos datos de entrenamiento, podemos sobreajustar seriamente el modelo.**\n",
    "\n",
    " * _Suposición básica_\n",
    "\n",
    "El principio de máxima verosimilitud es bastante intuitivo: estimar los parámetros de manera que se maximice la probabilidad de los datos. Esto trae consigo la suposición subyacente de que los parámetros **son fijos**, de manera que la incertidumbre proviene de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16960d2d",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "#### Refresh de matemáticas detrás del MLE\n",
    "\n",
    "Aquí la clave, es la palabara \"**maximizar**\". Queremos encontrar el valor de $\\theta$ que maximiza la función de verosimilitud $L(\\theta)$.\n",
    "\n",
    "Así que vamos a tener distintas partes importantes en el proceso para llegar a ese valor óptimo:\n",
    "\n",
    "1. Ya sabemos que si asumimos que los datos son _i.i.d._:\n",
    "\n",
    "$$\n",
    "X_1, X_2, \\dots, X_n \\overset{\\text{i.i.d.}}{\\sim} f(x; \\theta)\n",
    "$$\n",
    "\n",
    "Entonces la **función de verosimilitud** es:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n f(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "2. La **log-verosimilitud** es:\n",
    "\n",
    "Usualmente se trabaja con el **logaritmo** de la verosimilitud para facilitar el cálculo:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^n \\log f(x_i; \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943770a4",
   "metadata": {},
   "source": [
    "```{admonition} ¿Por qué usar la log-verosimilitud?\n",
    ":class: tip\n",
    "\n",
    "- Transformar productos $\\prod$ en sumas $\\sum$ *simplifica el cálculo*.\n",
    "- Hace más manejable la derivación, especialmente con distribuciones exponenciales.\n",
    "- Conserva la ubicación del máximo porque $\\log(\\cdot)$ es monótona creciente.\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe449aa",
   "metadata": {},
   "source": [
    "3. Encontrar el **máximo**:\n",
    "\n",
    "    a. Derivar la log-verosimilitud respecto del parámetro $\\theta$\n",
    "\n",
    "    b. Igualar a cero\n",
    "\n",
    "    c. Resolver para $\\theta$\n",
    "\n",
    "    d. $\\hat{\\theta} = \\arg\\max_{\\theta} \\ell(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b8b4d",
   "metadata": {},
   "source": [
    "```{admonition} Recuerda\n",
    ":class: tip\n",
    "\n",
    "* Si el modelo tiene _un solo parámetro_, se usa **cálculo diferencial** convencional:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} \\ell(\\theta) = 0\n",
    "$$\n",
    "\n",
    "* Si el modelo tiene _dos o más parámetros_:\n",
    "\n",
    "Se utilizan **derivadas parciales**, una por cada parámetro:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_1} \\ell(\\theta_1, \\theta_2) = 0\n",
    "\\qquad \\text{y} \\qquad\n",
    "\\frac{\\partial}{\\partial \\theta_2} \\ell(\\theta_1, \\theta_2) = 0\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2922bed",
   "metadata": {},
   "source": [
    "4. Verificar que es un máximo:\n",
    "\n",
    "a. Calcular la segunda derivada (o matriz Hessiana si hay varios parámetros).\n",
    "\n",
    "b. Dado que tenemos un problema en un dominio cerrado $0 \\leq \\theta \\leq 1$, podemos evaluar en los extremos:\n",
    "\n",
    "$$p(55 | \\theta=0) = p(55 | \\theta=1) = 0$$\n",
    "   \n",
    "y en el punto crítico: $\\hat{\\theta}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
