{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e44f84",
   "metadata": {},
   "source": [
    "# Sesión 5 A\n",
    "\n",
    "## El error (ε) en regresión lineal: de la geometría a la estadística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556a9a5",
   "metadata": {},
   "source": [
    "> **Objetivos de la clase:**\n",
    "> - Recordar el ajuste de curvas polinomiales.\n",
    "> - Entender el fenómeno de overfitting en casos prácticos.\n",
    "> - Explicar los mínimos cuadrados ordinaros mediante el principio de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcceaefb",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "Supongamos que tenemos un conjunto de entrenamiento con $N$ observaciones de $x$, \n",
    "\n",
    "$$[x_1, \\dots, x_N],$$\n",
    "\n",
    "en conjunto con las observaciones correspondientes de la variable objetivo $y$, \n",
    "\n",
    "$$[y_1, \\dots, y_N].$$\n",
    "\n",
    "En la siguiente gráfica mostramos datos de entrenamiento, con $N=20$. Estos datos se generaron eligiendo $x$ uniformemente espaciados en el intervalo $[0, 1]$, y la variable objetivo $y$ como el resultado de la función $\\sin (2 \\pi x)$ más un pequeño ruido distribuido normal:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9746ee",
   "metadata": {},
   "source": [
    "```{thebe-button}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siembra una semilla\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los datos (ficticios) del problema anterior\n",
    "N = 21\n",
    "\n",
    "x = np.linspace(0, 1, N)\n",
    "y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c042f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de los datos\n",
    "plt.plot(x, y, 'o', label='Datos', color='purple')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ec491",
   "metadata": {},
   "source": [
    "*Objetivo:* \n",
    "\n",
    "Explotar estos datos de entrenamiento para hacer predicciones $\\hat{y}$ de la variable objetivo para algún nuevo valor de la variable de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f999dcf",
   "metadata": {},
   "source": [
    "*Matemáticamente:*\n",
    "\n",
    "Antes de **formular el problema de forma probabilística**, procedamos de forma más intuitiva. Lo que queremos hacer es ajustar a los datos una función polinomial de la forma:\n",
    "\n",
    "$$\n",
    "f(x, w) = w_0 + w_1 x + w_2 x^{2} + \\dots + w_M x^{M} = \\sum_{j=0}^{M} w_j x^j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63454e26",
   "metadata": {},
   "source": [
    "Notemos que aunque $f$ es una función no lineal de $x$, es una función **lineal respecto a los coeficientes $w$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3706191",
   "metadata": {},
   "source": [
    "```{admonition} Linealidad del modelo (respecto a los parámetros)\n",
    ":class: tip\n",
    "\n",
    "**¿Qué significa que una función sea lineal respecto a sus parámetros?**\n",
    "\n",
    "Una función es *lineal en los parámetros* cuando los coeficientes aparecen *sin ser multiplicados entre sí, elevados a potencias ni dentro de funciones no lineales*.  \n",
    "\n",
    "En otras palabras, la expresión puede escribirse como:\n",
    "\n",
    "$$\n",
    "f(x; \\beta) = \\beta_0 g_0(x) + \\beta_1 g_1(x) + \\dots + \\beta_k g_k(x),\n",
    "$$\n",
    "\n",
    "donde $g_i(x)$ son funciones conocidas de las variables $x$.  \n",
    "\n",
    "- Ejemplo lineal en los parámetros:\n",
    "\n",
    "$$\n",
    "f(x; \\beta) = \\beta_0 + \\beta_1 x + \\beta_2 x^2,\n",
    "$$\n",
    "\n",
    "(lineal en $\\beta_0, \\beta_1, \\beta_2$, aunque no en $x$).  \n",
    "\n",
    "- Ejemplo **no** lineal en los parámetros:\n",
    "\n",
    "$$\n",
    "f(x; \\beta) = \\beta_0 + e^{\\beta_1 x},\n",
    "$$\n",
    "\n",
    "porque $\\beta_1$ aparece dentro de una función exponencial.  \n",
    "\n",
    "Esta distinción es crucial: si un modelo es lineal en los parámetros, se pueden usar métodos como *(OLS)* para estimarlos de forma directa.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbf459",
   "metadata": {},
   "source": [
    "Los valores de los *coeficientes (parámetros, pesos)* serán determinados *ajustando el polinomio a los datos de entrenamiento*. Esto se puede lograr minimizando una **función de error** (de costo o de pérdida) que mide la falta de ajuste entre la función $f(x, w)$ y los datos de entrenamiento.\n",
    "\n",
    "Una elección comúnmente usada para esta función de error está dada por la suma de cuadrados de los errores entre las predicciones sobre los datos de entrenamiento $f(x_n,w)$ y los valores correspondientes del objetivo $y_n$, de forma que minimizaremos:\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}\\left(f(x_n, w) - y_n\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b859b",
   "metadata": {},
   "source": [
    "De forma que podemos resolver el problema de ajuste de curvas mediante la elección de $w$ para la cual $E(w)$ sea lo más pequeña posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6e0b1",
   "metadata": {},
   "source": [
    "**Nota.** Dado que $E(w)$ es una función cuadrática de los coeficientes $w$, sus derivadas respecto a los coeficientes serán lineales respecto a $w$, y el problema de minimización tendrá solución única."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9debf2db",
   "metadata": {},
   "source": [
    "### Expresión matricial de la función objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57eed8e",
   "metadata": {},
   "source": [
    "Antes de continuar, conviene obtener una representación más compacta del problema anterior. Comencemos por trabajar con el polinomio, dándonos cuenta de que este es un producto punto entre los coeficientes $w$ y las potencias de $x$:\n",
    "\n",
    "\n",
    "$$\n",
    "f(x, w) = w_0 + w_1 x + w_2 x^{2} + \\dots + w_M x^{M}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "* $x$ es la variable explicativa (escalar),\n",
    "* $w_0, w_1, \\dots, w_M$ son los parámetros que queremos estimar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a3a3b",
   "metadata": {},
   "source": [
    "I. Vector de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7e30b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{#164ec6}{\\mathbf{w}} = \\underbrace{\\left[\\begin{array}{c} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M\\end{array}\\right]}_{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0fa64",
   "metadata": {},
   "source": [
    "II. Vector de características (o variables independientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32c806",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{#00b050}{\\phi(x)^\\top} = \n",
    "\\underbrace{[1 \\quad x \\quad x^2 \\quad \\dots \\quad x^M]}_{\\phi(x)^\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f4995",
   "metadata": {},
   "source": [
    "III. Producto escalar o producto punto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae8a62",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x; w) \\;=\\; \\textcolor{#00b050}{\\phi(x)^\\top} \\textcolor{#164ec6}{\\mathbf{w}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275adaaf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\color{purple}{\\Phi} =\n",
    "\\begin{bmatrix}\n",
    "\\textcolor{#00b050}{\\phi(x_1)^\\top} \\\\\n",
    "\\textcolor{#00b050}{\\phi(x_2)^\\top} \\\\\n",
    "\\vdots \\\\\n",
    "\\textcolor{#00b050}{\\phi(x_N)^\\top}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041a945",
   "metadata": {},
   "source": [
    "IV. Matriz de diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd1bda6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{#800080}{\\Phi}\\,\\textcolor{#164ec6}{\\mathbf{w}} \\;=\\;\n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_1^2 & \\dots & x_1^M \\\\\n",
    "1 & x_2 & x_2^2 & \\dots & x_2^M \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_N & x_N^2 & \\dots & x_N^M\n",
    "\\end{bmatrix}\n",
    "\\left[\\begin{array}{c} \n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "w_M\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18648eb1",
   "metadata": {},
   "source": [
    "V. Predicciones para todos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd063601",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf{y}} \\;=\\; \\Phi \\, \\mathbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3f44a",
   "metadata": {},
   "source": [
    "y, luego, ¿qué necesitamos hacer a continuación?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574e7b1",
   "metadata": {},
   "source": [
    "#### Norma euclidiana\n",
    "\n",
    "##### 1. La norma euclidiana: la idea general \n",
    "\n",
    "Primero recordemos la definición de la norma euclidiana, que simplemente mide la *\"longitud\"* de un vector en el espacio.\n",
    "\n",
    "Si tenemos un vector:\n",
    "\n",
    "$$\n",
    "v = \\left[\\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_s\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "la norma euclidiana de $v$ es:\n",
    "\n",
    "$$\n",
    "||v|| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_s^2},\n",
    "$$\n",
    "\n",
    "o equivalentemente $||v||^2 = v_1^2 + v_2^2 + \\dots + v_s^2 = \\sum_{i=1}^{s} v_i^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa54515",
   "metadata": {},
   "source": [
    "Con lo anterior, notemos que la función de error la podemos reescribir en términos de la norma de un vector:\n",
    "\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}\\left(f(x_n, w) - y_n\\right)^2 = \\frac{1}{2}\\sum_{n=1}^{N}\\left(\\phi(x_n)^T w - y_n\\right)^2 = \\frac{1}{2}||Z||^2,\n",
    "$$\n",
    "\n",
    "donde el vector $Z$ es:\n",
    "\n",
    "$$\n",
    "\\begin{align}\\nonumber\n",
    "Z & = & \\left[\\begin{array}{c} \\phi(x_1)^T w - y_1 \\\\ \\phi(x_2)^T w - y_2 \\\\ \\vdots \\\\ \\phi(x_N)^T w - y_N\\end{array}\\right] \\\\ \\nonumber\n",
    "       & = & \\underbrace{\\left[\\begin{array}{ccc} - & \\phi(x_1)^T & - \\\\ - & \\phi(x_2)^T & - \\\\ & \\vdots & \\\\ - & \\phi(x_N)^T & -\\end{array}\\right]}_{\\Phi} w - \\underbrace{\\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\\end{array}\\right]}_{y} \\\\ \\nonumber\n",
    "       & = & \\Phi w - y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93b3cd",
   "metadata": {},
   "source": [
    "De este modo, queremos encontrar\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\arg \\min_{w} \\frac{1}{2} ||\\Phi w - y||^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dea000",
   "metadata": {},
   "source": [
    "### Ejercicio numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolinomialFeatures y Phi (matriz de diseño)\n",
    "Phi = PolynomialFeatures(degree=2).fit_transform(x[:, None])\n",
    "Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un modelo usando Pipeline\n",
    "model_3 = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36202c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749eb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qué contiene x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera una partición entre train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:, None], y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo\n",
    "model_3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65807ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera los coeficientes de la regresión lineal\n",
    "model_3.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f80bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿cuál es el score sobre los datos de entrenamiento?\n",
    "model_3.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47478c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿cuál es el score sobre los datos de test?\n",
    "model_3.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, así como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_3.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_3.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4317b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un nuevo modelo con grado 10\n",
    "model_10 = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(degree=10)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo anterior\n",
    "model_10.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿cuál es el score sobre los datos de entrenamiento?\n",
    "model_10.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿cuál es el score sobre los datos de test?\n",
    "model_10.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, así como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_10.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_10.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1e6bf",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5eb4ef",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos definido la función de error como\n",
    "\n",
    "$$\n",
    "\\textcolor{#960096}{E(\\mathbf{w}) \\;=\\; \\tfrac{1}{2}\\|\\Phi \\mathbf{w} - \\mathbf{y}\\|^2},\n",
    "$$\n",
    "\n",
    "donde la norma euclidiana mide el tamaño del vector de errores \n",
    "$\\mathbf{e} = \\Phi \\mathbf{w} - \\mathbf{y}$.\n",
    "\n",
    "👉 Sin embargo, **aún no hemos puesto ninguna restricción sobre los parámetros** \n",
    "$\\mathbf{w}$.  \n",
    "Si los pesos crecen demasiado (por ejemplo, al intentar ajustar exactamente todos los puntos de entrenamiento), podemos caer en **overfitting**: el modelo memoriza el ruido en lugar de generalizar.\n",
    "\n",
    "Para evitarlo, añadimos un nuevo término a la función objetivo que ahora también \n",
    "mida la magnitud de los parámetros. Así obtenemos la **regresión Ridge**:\n",
    "\n",
    "$$\n",
    "E_{\\text{ridge}}(\\mathbf{w}) \n",
    "= \\textcolor{#960096}{\\tfrac{1}{2}\\|\\Phi \\mathbf{w} - \\mathbf{y}\\|^2}\n",
    "+ \\textcolor{#164ec6}{\\tfrac{\\lambda}{2}\\|\\mathbf{w}\\|^2},\n",
    "$$\n",
    "\n",
    "donde el hiperparámetro $\\lambda \\geq 0$ controla el equilibrio entre:\n",
    "\n",
    "- **Buen ajuste a los datos** (primer término).  \n",
    "- **Mantener los parámetros pequeños** (segundo término).  \n",
    "\n",
    "- Si $\\lambda = 0$, recuperamos la regresión normal.  \n",
    "- Si $\\lambda$ es grande, los pesos se reducen mucho y el modelo se vuelve más simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694ac03",
   "metadata": {},
   "source": [
    "¿qué solución óptima nos ofrece la regresión Ridge?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f751cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un pipeline, incluyendo ahora Ridge\n",
    "model_10_ridge = Pipeline([\n",
    "    (\"features\", PolynomialFeatures(degree=10)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", Ridge(alpha=1e-3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta el modelo\n",
    "model_10_ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15157f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿cuál es el score sobre los datos de entrenamiento?\n",
    "model_10_ridge.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿cuál es el score sobre los datos de test?\n",
    "model_10_ridge.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observa los coeficientes\n",
    "model_10_ridge.named_steps['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9958be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, así como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_10_ridge.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_10_ridge.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2f285",
   "metadata": {},
   "source": [
    "*¿qué pasa si incrementamos la cantidad de datos de entrenamiento?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera N=201\n",
    "\n",
    "N = 201\n",
    "x = np.linspace(0, 1, N)\n",
    "y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica la nube de puntos\n",
    "plt.plot(x, y, 'o', label='Datos', color='purple')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa en train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:, None], y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "model_10_ridge.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56927441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara los scores de train y test\n",
    "train_score = model_10_ridge.score(x_train, y_train)\n",
    "test_score = model_10_ridge.score(x_test, y_test)\n",
    "\n",
    "print(f\"Train score: {train_score:.3f}, Test score: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bcb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica los resultados de train y test, así como el polinomio\n",
    "plt.plot(x_train[:, 0], y_train, 'ob', label='Datos de entrenamiento')\n",
    "plt.plot(x_test[:, 0], y_test, 'og', label='Datos de prueba')\n",
    "x_model = np.linspace(0, 1, 100)\n",
    "y_model = model_10_ridge.predict(x_model[:, None])\n",
    "plt.plot(x_model, y_model, '-r', label='Modelo')\n",
    "plt.plot(x_test[:, 0], model_10_ridge.predict(x_test), '*r', label='Predicciones')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1da90f",
   "metadata": {},
   "source": [
    "```{admonition} Sobre overfitting\n",
    ":class: tip\n",
    "\n",
    "![](../images/sesion5_regularizacion1.png)\n",
    "\n",
    "**Figura 1**: Lo que muestra es cómo se ven las “bolas de restricción” (constraint balls) en 3D para distintos tipos de regularización: Ridge “encoge” los coeficientes pero difícilmente los lleva exactamente a cero; En Lasso, algunos coeficientes se vuelven exactamente cero; Elastic Net combina ambas propiedades. Hastie, *et al.*, 2020; Disponible en: https://arxiv.org/html/2006.00371v2. \n",
    "\n",
    "![](../images/sesion5_overfitting1.png)\n",
    "\n",
    "- **Punto rojo (OLS)**: mínimo sin regularización (en el centro de los contornos).  \n",
    "\n",
    "- **Punto verde (óptimo)**: solución con regularización:  \n",
    "\n",
    "  - **Ridge (L2)**:  \n",
    "    $w^* \\;=\\; \\frac{\\mu}{1 + \\lambda_2}$ → los coeficientes se *encogen* hacia el origen (**shrinkage**).  \n",
    "\n",
    "  - **Lasso (L1)**:  \n",
    "    $w^* \\;=\\; \\text{soft}(\\mu, \\tfrac{\\lambda_1}{2})$ → algunos coeficientes se reducen a 0 (*sparsity*).  \n",
    "\n",
    "  - **Elastic Net (L1+L2)**:  \n",
    "    $w^* \\;=\\; \\frac{1}{1 + \\lambda_2}\\,\\text{soft}(\\mu, \\tfrac{\\lambda_1}{2})$ → combinación: *shrinkage* (L2) + *sparsity* (L1).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c6f46",
   "metadata": {},
   "source": [
    "Para más información acerca de regularización y overfitting:\n",
    "\n",
    "Hastie, T. (2020). *Ridge Regularization: an Essential Concept in Data Science.* Stanford University. [arXiv](https://arxiv.org/html/2006.00371v2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf7cfe",
   "metadata": {},
   "source": [
    "## Perspectiva probabilística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48954c9d",
   "metadata": {},
   "source": [
    "En OLS definimos el error de ajuste de manera puramente geométrica, como la suma de cuadrados de los residuos.  \n",
    "\n",
    "En cambio, en el enfoque probabilístico de **Máxima Verosimilitud (MLE)**, partimos de un modelo:\n",
    "\n",
    "$$\n",
    "y = \\phi(x)^{\\top} w + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})\n",
    "$$\n",
    "\n",
    "Esto implica que, dado $x$, la variable de salida $y$ sigue una distribución normal:\n",
    "\n",
    "$$\n",
    "p(y \\mid x, w) = \\mathcal{N}\\big(y \\mid \\phi(x)^{\\top} w, \\, \\beta^{-1}\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4f319",
   "metadata": {},
   "source": [
    "![](../images/sesion5-modeldistr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10f46d",
   "metadata": {},
   "source": [
    "> **Figura 1:** Los puntos negros representan las observaciones $(x_i, y_i)$.  \n",
    "> La línea azul es el valor esperado del modelo $\\phi(x)^\\top w$.  \n",
    "> Las curvas rojas muestran la distribución empírica de los residuos, mientras que las curvas turquesa representan la distribución normal teórica asumida $\\mathcal{N}(0, \\beta^{-1})$.  \n",
    "> La figura ilustra que $p(y \\mid x,w) \\sim \\mathcal{N}(\\phi(x)^\\top w, \\beta^{-1})$, es decir, los datos se distribuyen alrededor de la recta con ruido gaussiano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7578e7",
   "metadata": {},
   "source": [
    "Asumiendo independencia entre las observaciones, la **función de verosimilitud** es:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = \\prod_{i=1}^N \\mathcal{N}(y_i \\mid \\phi(x_i)^{\\top} w, \\beta^{-1})\n",
    "$$\n",
    "\n",
    "y su logaritmo:\n",
    "\n",
    "\\begin{align}\n",
    "     & = \\sum_{i=1}^{N} \\log\\mathcal{N}(y_i | \\phi(x_i)^T w, \\beta^{-1}) \\\\\n",
    "     & = \\frac{N}{2}\\log\\beta - \\frac{N}{2}\\log(2 \\pi) - \\frac{\\beta}{2} \\sum_{i=1}^{N} (y_i - \\phi(x_i)^T w)^2 \\\\\n",
    "     & = \\frac{N}{2}\\log\\beta - \\frac{N}{2}\\log(2 \\pi) - \\frac{\\beta}{2} \\left|\\left|y - \\Phi w\\right|\\right|^2\n",
    "\\end{align}\n",
    "\n",
    "Finalmente, maximizar $\\ell(w)$ respecto a $w$ es equivalente a minimizar:\n",
    "\n",
    "$$\n",
    "\\hat{w}_{MLE} = \\arg \\max_{w} l(w) = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2\n",
    "$$\n",
    "\n",
    "que es exactamente el mismo criterio de **OLS**.  \n",
    "\n",
    "Así, OLS surge como un **caso particular de MLE** bajo el supuesto de ruido gaussiano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c6caa",
   "metadata": {},
   "source": [
    "![](../images/sesion5_mle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a137e",
   "metadata": {},
   "source": [
    "![](../images/sesion5_mle2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019dcd3",
   "metadata": {},
   "source": [
    "Observamos que la estimación de parámetros por máxima verosimilitud, explica nuestra intuición detrás de mínimos cuadrados.\n",
    "\n",
    "Además, **una vez más concluimos que el enfoque de máxima verosimilitud nos puede traer problemas de overfitting**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
